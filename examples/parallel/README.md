# llama.cpp/example/parallel

[parallel.cpp](parallel.cpp) Simplified simulation of serving incoming requests in parallel.

[distributed.c](distributed.c) Several independent LLMs that broadcast then reduce prompts put to them.
